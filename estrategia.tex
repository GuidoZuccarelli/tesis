\chapter{Enfoque General}
\label{chapter:estrategia}

Lo que vamos a construir es un caso de aplicación de la web semantica....

\section{Aplicaciones en la Web Semantica}
\section{Principios}
Dar historia (muy breve), definiciones  (RDF, tripletas, owl, inferencia, ..)
Escribir a medida que lo necesitas-.

Resource Description Framework (RDF)
Es una familia de especificaciones de WC3 diseñada para modelar datos intercambiables en la web. 
Consta de expresiones hechas por tripletas. Cada tripleta es considerada una sentencia para este lenguaje.
Una tripleta es un conjunto de un valor de cada uno de los siguientes tipos:

Recurso - Todo aquello descripto por RDF se lo denomina recurso, este último podría ser por ejemplo una página web entera (por ejemplo 
un documento HTML "http://www.w3.org/Overview.html") o una parte de ella.  Un recurso puede ser también un objeto que no 
se puede acceder directamente a través de la Web; por ejemplo un libro impreso. Los recursos son siempre nombrados por las 
URIs más identificadores de anclaje opcionales (ver [URI]). Cualquier cosa puede tener un URI; la extensibilidad de URIs 
permite la introducción de identificadores para cualquier entidad imaginable.

Propiedad - Una propiedad es un aspecto específico, característica, atributo o relación se utiliza para describir un recurso. 
Cada propiedad tiene un significado específico, define sus valores permitidos, los tipos de recursos que se puede describir, 
y su relación con otras propiedades. 

Objecto - puede ser otro recurso o puede ser un literal; es decir, un recurso (especificado por un URI) o una cadena simple o 
de otro tipo de datos primitivo definido por XML. En términos RDF, un literal puede tener contenido que es el formato XML,
pero no se evalúa aún más por el procesador de RDF.

Esta estructura forma una vinculación dirigida de un gráfico etiquetado, donde las aristas representan el enlace entre dos 
recursos (propiedad), representados por los nodos del grafo.

Su representación puede ser asociada tanto a un modelo entidad-relación como a un diagrama de clases de objetos.
Las Propiedades RDF pueden ser considerados como atributos de los recursos y en este sentido corresponden a pares 
atributo-valor tradicionales. Las Propiedades RDF también representan las relaciones entre los recursos y un modelo RDF​​, 
por tanto, pueden parecerse a los de un diagrama entidad-relación.
En la terminología de diseño orientado a objetos, los recursos corresponden a los objetos y propiedades 
corresponden a las variables de instancia.

RDF Schema (RDFS)

Es una extensión semántica de RDF, que provee elementos básicos para la descripción de ontologías (también llamadas vocabularios RDF) con el objetivo
de estructurar los recursos RDF. Proporciona la manera de definir udefinir las clases y las propiedades específicas de la aplicación en RDF.
RDFS no proporciona las clases sino que proporciona el marco necesario para poder definirlas.

Clases en RDFS se parecen mucho a las clases en lenguajes orientados a objetos. Esto permite que los recursos se definan como 
instancias de clases y subclases de las clases.

Elementos de RDFS

Básicos

    rdfs:Class permite declarar recursos como clases para otros recursos. 
    
    Atravez de la propiedad rdf:type, se puede asignar un tipo al recurso, un rucurso de tipo rdfs:Class puede entonces ser tipo de otro recurso. 
    Por ejemplo, podemos definir el recurso Auto como un rdfs:Class, y a su vez podemos definir el recurso Volkswagen Gol con la propiedad rdf:type 
    y siendo el objeto de esa propiedad el recurso Auto, entonces el tipo de Volkswagen Gol será auto.
    
    La defición de rdfs:Class es recursiva: rdfs:Class es la rdfs:Class de cualquier rdfs:Class.

    rdfs:Resource es la clase a la que pertenecen todos los recursos.

    rdfs:Literal es la clase de todos los valores literales, cadenas y enteros.

    rdfs:Datatype es la clase que abarca los tipos de datos definidos en el modelo RDF.

    rdfs:Property es la clase que abarca las propiedades.

Definen relaciones

    rdfs:subClassOf es una instancia de rdf:Property (osea una propiedad) que permite definir jerarquías. Relaciona una clase con sus superclases.

    rdfs:subPropertyOf es una instancia de rdf:Property que permite definir jerarquías de propiedades.
    
Restricciones de Propiedades

    rdfs:domain es una instancia de rdf:Property que especifica el dominio de una propiedad P. Esto es, la clase de los recursos que aparecen como sujetos en las tripletas donde P es predicado.

    rdfs:range es una instancia de rdf:Property que especifica el rango de una propiedad P. Esto es, la clase de los recursos que aparecen como objetos en las tripletas donde P es predicado.
    

SPARQL
Se trata de un lenguaje estandarizado de consula de grafos RDF.
SPARQL se puede utilizar para expresar consultas que permiten interrogar diversas fuentes de datos, si los datos se almacenan de forma nativa 
como RDF ​​o son definidos mediante vistas RDF ​​a través de algún sistema middleware. SPARQL contiene las capacidades para la 
consulta de los patrones obligatorios y opcionales de grafo, junto con sus conjunciones y disyunciones. SPARQL también 
soporta la ampliación o restricciones del ámbito de las consultas indicando los grafos sobre los que se opera. Los resultados 
de las consultas SPARQL pueden ser conjuntos de resultados o grafos RDF.


Términos de conjuntos de colecciones de datos semánticos

Model: Colección de sentencias.

DataSource: Colección de Modelos. Siendo uno de ellos el modelo por defecto y el resto modelos con nombre. El Datasource es de lectura y escritura, por lo que se pueden agregar tripletas.

Dataset: Lo mismo que el DataSource pero estático, por lo que es de solo lectura.

Graph: Colección de tripletas. Cualquier modelo puede ser transformado a un grafo, y así tener una representación más cercana de RDF.

DatasetGraph: Un contenedor de Grafos, similar al DataSource (También de lectura y escritura) que provee una estructura para un grafo por defecto y grafos con nombre.


HTML semántico

El uso y participación en la Web Semántica a travez de documentos RDF no es atractivo para la mayoría, porque no parece proveer un beneficio 
directo, ya que esto implica, aprender un lenguaje nuevo para generar documentos que sólo le será de utilidad a usuarios con un grado de conocimiento 
muy alto. 

Para resolver este problema, existen etiquetas de marcado en HTML que le proporcionan información semántica al documento, que copmo se verá más adelante 
puede ser extraída para generar documentos RDF. Existen distintos tipos:

Microdatos: Son un grupo de pares nombre-valor anidados dentro del documento, en paralelo con el contenido existente. Dichos grupo son 
también llamados ítems y cada par nombre-valor es la propiedad. Los atributos más utilizados son:

Para crear un ítem, se utiliza el atributo de HTML ``itemscope``. 

Para crear una propiedad se utiliza el atributo ''itemprop``.

Luego para definir los valores, se utilizan según sea una URL o un String los siguientes atributos:
''a`` con ''href``, ''src`` o ''img`` para difinir un valor URL
''value'' para un String.

También se le puede establecer el tipo a un ítem mediante el atributo ''itemtype``


Microformatos: Nacen con la intención acercar más personas al uso de la web semántica a travez de una forma sensilla de utilizar 
una serie de atributos de las etiquetas de XHTML:

class: especifica el nombre de la clase

rel: Se utiliza en los enlaces para indicar relaciones en los documentos.

rev: Igual al rev pero en sentido inverso.

Con el uso del atributo class se puede entonces especificar un microformato a utilizar. Existen muchos tipos como por ejemplo: hCard (para describir 
personas), hreview (para describir reviews), geo (para describir posiciones), etc. Cada uno de ellos con distintas propiedades.


RDF-a: Permite directamente embeber tripletas (sujeto-predicado-objeto) 

\section{Tecnologías}
Triplestores, frameworks, extractores, crawlers, motores de busqueda semanticos...

\section{Estrategia propuesta}
Con el fin de crear una aplicación que satisfaga los requerimientos mensionados anteriormente, se debe encontrar
un procedimiento que incluya desde obtener los datos relevantes de la web hasta llevarlos a un estado que permita una 
explotación satisfactoria. 

El procedimiento debería incluir los siguientes pasos  

\begin{description}
\item[Selección de vocabularios ] Tanto la recolección como la publicación de datos en la web semántica involucra la elección del o de los vocabulario/s que mejor modelen el dominio de problema, con la excepción que para su publicación existe la posibilidad de desarrollar uno propio que se ajuste correctamente en caso de que no se encuentre uno existente. 
\item[Recolección y extracción de los datos ] .... ?
\item[Evaluación de Calidad de los Datos] ???
\item[Curado de los Datos] ???
\item[Integración de los datos] ?? 
\item[Publicación del Dataset Curado] ?? 
\item[Explotación del Dataset] ?? 
\end{description}

\begin{framed}
\textcolor{red}{acá sería bueno incluir un diagrama que muestre el pipeline. En la lista de arriba con un párrafo se explica cada paso. Luego, en las secciones que sigue se analiza mas cada paso, pero se lo plantea como un problema.. se analizan los retos; entonces los capítulos 4 a 9, explican como los resolviste. Alternativamente, se puede hablar menos acá, solo lo suficiente para que se entienda la estrategia general, y luego en los capítulos 4 a 9 se analiza el problema en detalle y se propone la solución. Con esta ultima forma, todo lo que se refiere a los vocabularios quedaría en el capitulo 4 y no acá. Puede ser mejor.}
\end{framed}


\section{Selección de vocabularios}

Seleccionar un vocabulario implica analizar varios aspectos del mismo, no sólo su definición e implementación, sino también el uso práctico dado por sus usuarios. 
En primer lugar se debe comprobar que los nombres de las propiedades que posee sean correctamente auto-explicativas. Supongamos por ejemplo que existe un ítem con un rating agregado modelado por una ontología que posee las siguientes propiedades:

\begin{enumerate}
\item minrating
\item ratingValue
\item countRating
\end{enumerate}


Las dos últimas propiedades resultan fácilmente identificables, ratingValue se trata del promedio de puntaje, y ratingCount 
la cantidad de puntajes que le fueron otorgados, pero la propiedad minRating podría generar distintas formas de interpretación, 
alguien podría suponer que se trata del valor mínimo que fue adquirido por un usuario, o el valor mínimo que un usuario puede 
otorgar. Y muchas veces la documentación de la ontología (si es que existe) no es suficiente.

Luego se deberá analizar si existen las propiedades para cubrir las necesidades mínimas de los casos de uso.

Y por último se debería intentar buscar ejemplos reales que muestren el uso que le dieron los usuarios a la ontología, para 
determinar qué propiedades están incorrectamente interpretadas o también para los casos donde las propiedades que se encuentren 
en desuso.

Con estas precauciones en mente se puede emprender la búsqueda, que podría tener como comienzo búsquedas en  search engines. 
Existen dos buscadores específicos para esta tarea:

\subsection{lov}

Linked Open Vocabulary (LOV)

Proporciona una plataforma técnica de búsqueda y evaluación de calidad sobre un dataset extraído de linked data cloud que contiene descripciones de vocabularios RDFS 
y ontologías OWL. Esas descripciones están en forma de metadatos y pueden ser generados tanto por los autores de los vocabularios como por curadores de LOV.
Posee además de la búsqueda las funciones de estadística o sugerencia.

Actualmente el dataset está integrado por 475 namespaces distintos que contienen una media de 10 clases y 20 propiedades, siendo schema.org el más grande de ellos.

\subsection{vocabcc}

Vocab.cc

Vocabcc es un proyecto opensource que permite a los desarolladores de RDF realizar búsquedas de vocabularios de Linked Data.

Para facilitar la decisión de seleccionar un vocabulario deseado, proporciona además información estadística de cada uno sobre el dataset Billon Triple Challenge (BTCD). 
 Esta información incluye el número de apariciones globales de la URI dada en el BTCD, así como el número de documentos dentro de la BTCD, que contiene el URI dado. Estos números permiten una clasificación de propiedades y clases, respectivamente, con respecto a su uso. También se proporciona información acerca de la posición de una URI dada en estos rankings.

Los desarrolladores pueden buscar las URIs con queries arbitrarias o búsquedas de URIs específicos (prefijos comunes se resuelven automáticamente con datos de prefix.cc).

Para permitir una fácil integración de la funcionalidad vocab.cc, toda la información está disponible como RDF y se puede acceder como Servicio Vinculado.

\section{Recolección y extracción de los datos}

%\input{explicacionReco}
Como se mencionó antes, la web contiene grandes cantidades de documentos publicados con información semántica. Pero la tarea
de encontrarlos, con el agregado de que sólo una pequeña porción de ellos será relevante para los requerimientos no es trivial
en lo absoluto debido a la inmensidad del universo en el que se encuentran. La forma de llevar a cabo este objetivo está 
atada al hardware disponible, tanto para almacenar los datos, como para el tiempo que va a emplear la ejecución de esta 
tarea.  

Dado que las bases de datos semánticas sólo almacenan información en forma de tripletas o cuadrupletas, los documentos encontrados 
deberán someterse a un proceso de extracción que seleccione las sentencias HTML y las convierta a alguno de los lenguajes que soportan  
tripletas o cuadrupletas. Para esto existen múltiples herramientas. 

Una vez encontrados, descargados y transformados los documentos HTML a documentos semánticos puede construirse la base de datos semántica 
con la información recolectada. 

Estos son los cuatro pasos necesarios para lograr tener el dataset semántico con el cual se puede empezar a trabajar. Cada uno de ellos 
posee distintas alternativas para su realización, algunas se describirán a continuación 


%\input{opcionesReco} vacio

\subsection{Búsqueda}
%\input{opcionesBusqueda}
\textcolor{red}{OJO: ESTO ESTABA EN EL ARCHIVO busqueda.tex no en el opcionesBusqueda}


La forma de ejecución de esta tarea dependerá de algunos aspectos:

Recursos de hardware disponibles

Cantidad y calidad de la información requerida

El grado de atemporalidad mínima tolerable en los datos

El primer paso para realizar la recolección es intentar responder la siguiente pregunta: ¿Dónde encuentro la informanción?

Una vez seleccionados los vocabularios, se necesitará obtener fuentes de datos que contengan sus datos publicados en esos vocabularios.
 
Para lograrlo se podrá utilizar como punto de partida:

Sitios indexadores: Son algunos sitios que disponen de un dataset muy grande procesado con documentos indexados, que ofrecen consultar dicho 
dataset mediante servicios web. Generalmente proveen una API donde se pueden consutlar los datos mediante distintos grados de flexibilidad.

Sindice, LOD cloud cache y UriBurner son algunos ejemplos de estos sitios. Se puede utilizar entocnes, los servicios web a fin
de obtener una lista de URL que se cree que tendrán la información necesaria.

Sindice: Es una herramienta creada en conjunto por la Universidad de Deri,  Fondazione Bruno Kessler y Openlink Software que propociona múltiples tipos de API ofreciendo acceso a su dataset de la web de datos. Este dataset contiene información recolectada de la web en múltiples formatos de la web semántica y puede ser accedido a travez de un search engine, una API restful o un SPARQL endpoint.  
En la actualidad posee indexados 708.26 millones de documentos.

Link Open Data Cloud Cache (LOD): Al igual que sindice proporciona acceso a un dataset recolectado de la web de datos pero de una manera mucho más acotada. Se dispone de un text search (funciona como un search engine) o de un SPARQL Endpoint bastante restringido. Sólo se puede acceder al dataset entero mediante federated queries, en caso de querer buscar sobre todo el dataset, el endpoint limita la consulta sólo a una parte del mismo. Tambien posee muchas restricciones respecto a los time outs, por lo que las consultas no pueden ser muy flexibles.
Para poder disponer de la funcionalidad completa del endpoint y así poder aprovechar tanto el dataset como las consultas SPARQL deberá comrprarse una licencia.

Sitios autoritativos: Son sitios conocidos que generan información relevante y la publican en las ontologías y vocabularios 
seleccionados. Ejemplos aplicados al caso de estudio podrían ser IMDB (que publica reviews de películas bajo el vocabulario schema), o Rottentomatoes
(que hace lo mismo pero no solo con películas). Se podrán utilizar entonces estos sitios como punto base para un posible crawling.

Catálogos de enpoints: Son catálogos provistos por algunos sitios que mantienen una lista actualizada de SPARQL endpoints 
junto con el estado de disponibilidad en el que se encuentran. Por ejemplo los sitios http://labs.mondeca.com/sparqlendpointsstatus/ y 
http://www.w3.org/wiki/SparqlEndpoints que este último además provee detalles sobre la información de los mismos.

Consultar estos sitios entonces generará una lista de endpoints SPARQL que se podrá utilizar para consultar la información necesaria.

Volcados de datos: Son datasets muy grandes que fueron el resultado de un web crawling, los cuales están disponibles para su descarga 
y se pueden utilizar para procesarlos y obtener los datos requeridos. El más importante es http://challenge.semanticweb.org/2014/ .



%\input{opcionesDescarga}
%input{opcionesTransformación}
%input{opcionesAlmacenaje}

 

\section{Evaluación de Calidad de los Datos} 
Como sabemos los reviews son creados por usuarios que en su mayoría no están familiarizados con el desarrollo de una 
aplicación, esto causa que una parte muy significativa del contenido publicado no esté de la forma adecuada para ser procesado. 
La falta de calidad en el contenido publicado puede deberse tanto a errores por parte del usuario como por parte del publicador.


El publicador deberá asegurarse que los datos respeten rigurosamente las ontologías en las cuales se publican.

El sitio web en el cual el usuario se encuentre realizando el review deberá guiarlo en todo lo posible para lograr que la evaluación
quede en un formato adecuado.


Aunque también existen muchos problemas que no dependen del sitio web, generalmente errores semánticos de calidad, donde lo 
redactado esta hecho de forma inconsistente o insuficiente.


Este paso entonces tiene por objetivo encontrar todos los problemas de calidad que pueda haber en el dataset que acaba de 
ser descargado y extraído y que generen inconvenientes para una posterior integración/explotación.

Por lo general se utilizará la base de datos mediante consultas SPARQL estratégicamente generadas para lograr sacar estadísticas y detectar los problemas.

Existen frameworks para realizar estas operaciones.

%\input{estraCur}
\section{Curado de los Datos}

En el paso anterior se describieron los problemas en la calidad del dataset que tendrán un impacto en la aplicación resultante, 
o que, podrían limitar o imposibilitar la realización de la misma. Dichos problemas pueden ser muchos, y algunos muy difíciles
de solucionar, será parte entonces del proceso, reconocer aquellos que su resolución sea viable y además encontrar la forma de 
implementarla.

Cabe destacar que el proceso de evaluar el dataset en búsqueda de problemas y implementar soluciones es iterativo, debido a que 
una mejora puede conllevar a la detección de nuevos problemas.

%\input{estraInt}

\section{Integración de los datos}
\begin{framed}
\textcolor{red}{Esto va acá o es parte de Explotación?}
\end{framed}


Los datos recolectados fueron generados por distintos usuarios, en múltiples sitios y bajo distintas ontologías y estándares.
Estos aún curados, necesitan un último paso para poder realizar una explotación, y es la integración.

Este gran y dificultoso proceso abarca cualquier operación que intente dar una visión más unificada de la información. Este
proceso puede tener distintos niveles y aspectos a integrar, como podría ser por ejemplo, unificar las ontologías de los reviews, o bien
en una nueva ontología de review, o bien en una ya existente, para lograr tener información semánticamente más parecida. Ya 
que este proceso puede ser muy dificultoso, habrá que ver en base a los requerimientos, qué aspectos de los datos se integrarán.

Una vez realizada la integración mínima necesaria para los requerimientos, el dataset ya estará listo para su explotación.

\section{Publicación del Dataset Curado}
???

\section{Explotación del Dataset}
\begin{framed}
\textcolor{red}{Acá es donde efectivamente se discute que implica cubrir los requerimientos de la aplicación . Puede ser que acá tambien te refieras a como la forma en la que querés explotar los datos impacta en las fases anteriores. Incluso, habría que pensarlo, en este capitulo tal vez conviene primero hablar de explotación y luego de las otras fases - porque explotación determina mucho que es lo que vas a mirar en las fases anteriores, ¿no?}
\end{framed}

