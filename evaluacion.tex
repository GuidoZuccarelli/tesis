\chapter{Evaluación de Calidad de los Datos}
\label{chapter:evaluacion}

Objetivo:
Encontrar problemas en el dataset que dificulten una posible integración y modifiquen el correcto resultado de una posible aplicación derivada.

Evaluación nº 1 - Vocabularios

Objetivo:
Buscar aquellas propiedades y clases ligadas a recursos relevantes (que estén relacionados con algún review mediante property paths) 
para las cuales no se encuentre representada en ninguna ontología. Y luego verificar que genere algún problema para la aplicación a construir.

Estrategia:
Este primer paso es bastante simple, básicamente se deben conseguir todos los vocabularios utilizados (para saber cuales son estos basta con revisar el namespace 
de los prefijos) y cargarlos en el dataset.
Una vez con los vocabularios cargados se puede realizar una consulta en SPARQL para las propiedades y leugo otra para las clases:

SELECT ?c (count (distinct ?o) as ?cantidad)
WHERE{
{
?s <http://local.org/id> ?id .
?s (:|!:)+ ?o .
}UNION{
?s <http://local.org/itemId> ?id .
?s (:|!:)+ ?o .
}
?o a ?c .
FILTER NOT EXISTS{
?c a rdfs:Class.
}
}GROUP BY ?c ORDER BY DESC(?cantidad)

Luego se puede hacer una consulta igual pero buscando por propiedades.

Resultados:
Los resultados de esta evaluación fueron interesantes. Además de algunas propiedades mal escritas, ya sea por una letra faltante o una minúscula en lugar 
de una mayúscula, se encontró que todas las propiedades utilizadas de la ontología schema estaban incorrectas.


Evaluación nº 2 - Duplicados

Objetivo:
Disponiendo de un gran dataset de información recolectada de la web, un paso muy útil es limpiar aquellos datos innecesarios. 
Este paso consiste en detectar aquellos reviews en el dataset que se encuentren duplicados.

Estrategia:
La forma más sencilla que a cualquiera normalmente se le ocurriría es comparar los reviews todos con todos. Cuyo algoritmo tiene orden cuadrático que para el tamaño del dataset resulta demasiado denso.
El algoritmo que se utilizó fue el siguiente:
1)Por cada ontología de review se escogió arbitrariamente las propiedades más utilizadas y representativas de cada una:

Purl-Review
dcterms:date
rev:text
rev:title
rev:rating

Schema-Review
sorg:datePublished
sorg:description
sorg:name
sorg:author
sorg:reviewBody

Purl-ReviewAggregate
revagg:average
revagg:count
dcterms:date
rev:title

Schema-AggregateRating
sorgagg:ratingCount
sorgagg:ratingValue
sorgagg:reviewCount

Luego se arma en cada ontología un mapa para cada propiedad escogida, que contiene como clave un valor posible existente para esa propiedad, y 
como valor un arreglo con todos los reviews que contiene ese valor para dicha propiedad.
En otras palabras se separaron por cada propiedad, los reviews según su valor para esa propiedad.
Luego se buscaron qué grupos de reviews se encontraban juntos para más de una propiedad mendiante obtener la intersección de ambos conjuntos. Por ejemplo:

Para el valor 4 de la propiedad rev:rating se encuentran {review1, review2, review3}
Y para el valor ``buena película'' de la propiedad rev:text se encuentran {review2, review3, review4}
Entonces la intersección entre el conjunto de reviews para el primer conjunto con el segundo conjunto es {review2, review3}.
Ahora para marcar ese conjunto como posible grupo de duplicados deben cumplir la condición de que, el ítem sobre el cuál el review habla
tiene el mismo nombre en cada uno de los elementos.

Esta comparación se reliza entre todos los conjuntos de reviews de los valores de una propiedad con el resto de los conjuntos de los valores de las restantes propiedades de esa ontología.

Una vez obtenidos los conjuntos de los posibles duplicados se procede ahora sí a analizar minuciosamente los reviews todos con todos pero dentro del conjunto marcado como posible grupo de duplicados
para corroborar.

Resultados:

Se encontraron en total 17343 reviews replicados en un total de 78705 reviews de los cuales.

1009 estaban duplicados dentro de un mismo documento.
64991 estaban duplicados entre distintos documento pero dentro de un mismo dominio
12709 estaban duplicados en documentos pertenecientes a distintos dominios

Ésta es la lista con los dominios a los cuales se le encontraron más cantidad de reviews duplicados:


\begin{tabular}{| l | c | }
Dominio & Cantidad de reviews duplicados \\
4outof10.com &	21582\\
www.superpages.com &	18756\\
www.kollermedia.at &	9864\\
www.realtruck.com &	8372\\
ormigo.com &	8133\\
www.reptilecentre.com &	3440\\
www.querfood.de &	3275\\
www.carsurvey.org &	2766\\
www.chip.de &	1526\\
www.thewinecellarinsider.com &	1491\\
\end{tabular}


%Estrategia:

%Como base se utilizó el framework RDFUnit que provee funcionalidades que asisten tanto a la búsuqeda de problemas sintácticos como semánticos.

%Para encontrar los errores sintácticos se utilizaró una herramienta del framework que genera test automáticos sobre un vocabulario, y ésto se realizó sobre PURL y SCHEMA.

%RDFUnit también dispone de una serie de patrones de tests que pueden ser aplicados al contexto e implementados, algunos de los cuales pueden ser utilizados para detectar errores semánticos. 

