\chapter{Evaluación de Calidad de los Datos}
\label{chapter:evaluacion}

Objetivo:
Encontrar problemas en el dataset que dificulten una posible integración y modifiquen el correcto resultado de una posible aplicación derivada.

Evaluación nº 1 - Vocabularios

Objetivo:
Buscar aquellas propiedades y clases ligadas a recursos relevantes (que estén relacionados con algún review mediante property paths) 
para las cuales no se encuentre representada en ninguna ontología. Y luego verificar que genere algún problema para la aplicación a construir.

Estrategia:
Este primer paso es bastante simple, básicamente se deben conseguir todos los vocabularios utilizados (para saber cuales son estos basta con revisar el namespace 
de los prefijos) y cargarlos en el dataset.
Una vez con los vocabularios cargados se puede realizar una consulta en SPARQL para las propiedades y leugo otra para las clases:

SELECT ?c (count (distinct ?o) as ?cantidad)
WHERE{
{
?s <http://local.org/id> ?id .
?s (:|!:)+ ?o .
}UNION{
?s <http://local.org/itemId> ?id .
?s (:|!:)+ ?o .
}
?o a ?c .
FILTER NOT EXISTS{
?c a rdfs:Class.
}
}GROUP BY ?c ORDER BY DESC(?cantidad)

Luego se puede hacer una consulta igual pero buscando por propiedades.

Resultados:
Los resultados de esta evaluación fueron interesantes. Además de algunas propiedades y clases mal escritas, ya sea por una letra faltante o una minúscula en lugar 
de una mayúscula, se encontró que todas las propiedades utilizadas de la ontología schema estaban incorrectas.

Ninguna de las propiedades encontradas bajo el namespace http://schema.org/ pertenecían a la ontología schema.
Esto se debe a que cada propiedad incluía bajo su path su clase dominio de la siguiente manera: http://schema.org/CLASE/Propiedad .
Por ejemplo:
http://schema.org/Product/aggregateRating en lugar de http://schema.org/aggregateRating

Situaciones como ésta generan problemas cuando se intenta realizar consultas generales. Si por ejemplo se requiriera encontrar todos los aggregateRatings de todos los recursos no alcanzaría con 
una simple querie, habría que realizar tantas queries como clases de la ontología schema existan.

Haciendo un recorrido sobre los datos desde su origen (los documentos html publicados) hasta este punto, se encontró que el problema se 
originó en el proceso de extracción. 
Any23 cuando extrae las propiedades de la ontología schema, modifica las mismas adicionando el nombre de la clase de su correspondiente dominio, provocando que éstas queden incorrectas.

Evaluación nº 2 - Duplicados

Objetivo:
Disponiendo de un gran dataset de información recolectada de la web, un paso muy útil es limpiar aquellos datos innecesarios. 
Este paso consiste en detectar aquellos reviews en el dataset que se encuentren duplicados.

Estrategia:
La forma más sencilla que a cualquiera normalmente se le ocurriría es comparar los reviews todos con todos. Cuyo algoritmo tiene orden cuadrático que para el tamaño del dataset resulta demasiado denso.
El algoritmo que se utilizó fue el siguiente:
1)Por cada ontología de review se escogió arbitrariamente las propiedades más utilizadas y representativas de cada una:

Purl-Review
dcterms:date
rev:text
rev:title
rev:rating

Schema-Review
sorg:datePublished
sorg:description
sorg:name
sorg:author
sorg:reviewBody

Purl-ReviewAggregate
revagg:average
revagg:count
dcterms:date
rev:title

Schema-AggregateRating
sorgagg:ratingCount
sorgagg:ratingValue
sorgagg:reviewCount

Luego se arma en cada ontología un mapa para cada propiedad escogida, que contiene como clave un valor posible existente para esa propiedad, y 
como valor un arreglo con todos los reviews que contiene ese valor para dicha propiedad.
En otras palabras se separaron por cada propiedad, los reviews según su valor para esa propiedad.
Luego se buscaron qué grupos de reviews se encontraban juntos para más de una propiedad mendiante obtener la intersección de ambos conjuntos. Por ejemplo:

Para el valor 4 de la propiedad rev:rating se encuentran {review1, review2, review3}
Y para el valor ``buena película'' de la propiedad rev:text se encuentran {review2, review3, review4}
Entonces la intersección entre el conjunto de reviews para el primer conjunto con el segundo conjunto es {review2, review3}.
Ahora para marcar ese conjunto como posible grupo de duplicados deben cumplir la condición de que, el ítem sobre el cuál el review habla
tiene el mismo nombre en cada uno de los elementos.

Esta comparación se reliza entre todos los conjuntos de reviews de los valores de una propiedad con el resto de los conjuntos de los valores de las restantes propiedades de esa ontología.

Una vez obtenidos los conjuntos de los posibles duplicados se procede ahora sí a analizar minuciosamente los reviews todos con todos pero dentro del conjunto marcado como posible grupo de duplicados
para corroborar.

Resultados:

Se encontraron en total 17343 reviews replicados en un total de 78705 reviews de los cuales.

1009 estaban duplicados dentro de un mismo documento.
64991 estaban duplicados entre distintos documento pero dentro de un mismo dominio
12709 estaban duplicados en documentos pertenecientes a distintos dominios

Ésta es la lista con los dominios a los cuales se le encontraron más cantidad de reviews duplicados:


\begin{tabular}{| l | c | }
Dominio & Cantidad de reviews duplicados \\
4outof10.com &	21582\\
www.superpages.com &	18756\\
www.kollermedia.at &	9864\\
www.realtruck.com &	8372\\
ormigo.com &	8133\\
www.reptilecentre.com &	3440\\
www.querfood.de &	3275\\
www.carsurvey.org &	2766\\
www.chip.de &	1526\\
www.thewinecellarinsider.com &	1491\\
\end{tabular}

Evaluación nº 3 - RDFUnit-Automatizado

Objetivo:
Encontrar todos los problemas sintácticos de los datos relevantes del dataset e identificar cuales pueden representar un problema 
para construir la aplicación.

Estrategia:
El framework crea test automaticamente a partir de las ontologías, buscando una amplia variedad de problemas posibles. Por lo que se 
instaló el framework, se lo proveyó de las ontologías relevantes y se corrieron los test sobre el dataset.

Resultados:

\begin{tabular}{| l | c | }
Total test cases & 2116\\
Succeeded & 2067\\
Failed & 49\\
\end{tabular}

Los tests fallados más relevantes fueron para schema:

\begin{tabular}{| l | c | r | }
http://schema.org/datePublished does not have datatype: http://www.w3.org/2001/XMLSchema\#date & 152138 & 152138\\
http://schema.org/ratingCount does not have datatype: http://www.w3.org/2001/XMLSchema\#decimal & 12509 & 12509\\
http://schema.org/ratingValue has rdfs:domain different from: http://schema.org/Rating & 6198	 & 181178\\
http://schema.org/worstRating has rdfs:domain different from: http://schema.org/Rating & 5867 & 108095\\
http://schema.org/bestRating has rdfs:domain different from: http://schema.org/Rating & 5867 & 122057\\
http://schema.org/aggregateRating is missing proper range & 4984 & 56890\\
http://schema.org/reviewRating has different range from: http://schema.org/Rating & 4095 & 144239\\
http://schema.org/name does not contain a literal value (http://www.w3.org/2001/XMLSchema\#string) & 1310 & 190840\\
http://schema.org/itemReviewed is missing proper range & 1058 & 1505\\
http://schema.org/reviewRating has rdfs:domain different from: http://schema.org/Review & 331	 & 144239\\
http://schema.org/email does not contain a literal value (http://www.w3.org/2001/XMLSchema\#string) & 174 & 174\\
\end{tabular}

Para purl:

\begin{tabular}{| l | c | r | }
http://purl.org/stuff/rev\#reviewer is missing proper range & 83340 & 157004\\
http://purl.org/stuff/rev\#reviewer has different range from: http://xmlns.com/foaf/0.1/Person	 & 73664 & 157004\\
http://purl.org/stuff/rev\#hasReview has different range from: http://purl.org/stuff/rev\#Review & 72398 & 347548\\
http://purl.org/stuff/rev\#title has rdfs:domain different from: http://purl.org/stuff/rev\#Review & 14705 & 104900\\
\end{tabular}

Evaluación nº 4 - RDFUnit-Manual

Objetivo: Correr test manuales en RDFUnit para evaluar el dataset en busca de errores sintácticos no contemplados por los test
automáticos de la evaluación anterior.

Estrategia: Se configuró RDFUnit utilizando patrones del framework para crear tests manuales. Luego se corrió para obtener resultados
de la misma manera que en la evaluación anterior

Resultados:
\begin{tabular}{| l | c | r | }
http://purl.org/dc/terms/date has a format different to YYYY-MM-DD & 92580 & 207130\\
http://schema.org/datePublished has a format different to YYYY-MM-DD & 53740 & 144594\\
http://schema.org/publishDate has a format different to YYYY-MM-DD & 50070 & 59825\\
http://schema.org/dtreviewed has a format different to YYYY-MM-DD & 26903 & 26903\\ 
http://purl.org/stuff/rev\#rating is not in the expected range (1-5) & 15319 & 298458\\
http://purl.org/stuff/rev\#rating is not a natural number & 2866 & 301322\\
http://schema.org/ratingValue is smaller than http://schema.org/worstRating & 41 & 108088 \\
http://schema.org/ratingValue is not a natural number & 33 & 60682\\
http://schema.org/ratingValue is not in the expected range (1-5) & 1 & 73090 
\end{tabular}


%Estrategia:

%Como base se utilizó el framework RDFUnit que provee funcionalidades que asisten tanto a la búsuqeda de problemas sintácticos como semánticos.

%Para encontrar los errores sintácticos se utilizaró una herramienta del framework que genera test automáticos sobre un vocabulario, y ésto se realizó sobre PURL y SCHEMA.

%RDFUnit también dispone de una serie de patrones de tests que pueden ser aplicados al contexto e implementados, algunos de los cuales pueden ser utilizados para detectar errores semánticos. 

