\chapter{Curado de los Datos}
\label{chapter:curado}

Objetivo:
Corregir todos los problemas viables encontrados en el paso anterior que se pueda.

Curado Evaluación nº 1 - Vocabularios:

Estrategia:
Se optó por adicionar al review la propiedad en forma correcta con el mismo recurso destino sin eliminar la propiedad incorrecta. 
Todo se realizó con una simple consulta SPARQL
\begin{lstlisting}[frame=single]  
INSERT{
  GRAPH ?g {
    ?s ?p ?o .
  }
}
WHERE{
  SELECT ?g ?s (IRI(CONCAT("http://schema.org/", ?prop)) AS ?p) ?o
  WHERE{
    GRAPH ?g{
      ?s ?t ?o .
      FILTER(REGEX(str(?t), "http://schema.org/*+", "i")).
      BIND(REPLACE(str(?t), '^.*(\#|/)', "") AS ?prop)
    }
  }
}
\end{lstlisting}


Resultados:
Se crearon 2498221 propiedades nuevas de forma correcta correspondientes a cada una de las propiedades incorrectas de la ontología 
schema.

Curado Evaluación nº 2 - Duplicados

Estrategia: 

Se recorrieron todos los grupos de duplicados y se procedió a realizar la siguiente operación:
Se le asignó a cada grupo un id único, al cual llamaremos groupId.
Lueog por cada grupo de duplicados se seleccionó de manera arbitraria un representante con un criterio que se describirá a continuación:
Si dentro del conjunto de documentos entre los cuales se encuentran todos los integrantes del grupo de duplicados no existe ninguno 
que contenga un recurso ya elegido anteriormente como representante, se selecciona cualquier recurso de la lista de duplicados al azar.
Caso contrario, se selecciona el primer recurso encontrado en la lista, que pertenezca al documento que ya contiene algún recurso escogido 
como representante.
Esto se realizó de esa forma, para que todos los representantes se concentren en los mismos documentos y no suceda que dados 2 
documentos idénticos con 2 reviews cada uno, no terminen con un representante cada uno. De esta forma se minimizan la cantidad de 
documentos con reviews en el dataset.
Luego de elegir un representante para el grupo de duplicados, a ese recurso se le asignó una propiedad http://local.org/representantOf y 
como objeto de la propiedad el groupId.
Y para todos los recursos restantes del grupo de duplicados, se les asignó una propiedad http://local.org/duplicatedOf con objeto 
el groupId.

Resultados:

Se asignaron 78705 propiedades duplicatedOf y 17343 representantOf

Curado Evaluación nº 3 - RDFUnit Automático:

Muchos de los problemas detectados por el framework, no otorgaban demasiada información, como por ejemplo
http://schema.org/datePublished does not have datatype: http://www.w3.org/2001/XMLSchema\#date el cual sólo especifica que le 
falta el tipo al recurso objeto, pero no necesariamente significa que esté mal, esto da un indicio de que algo puede andar mal
y será verificado en la consulta manual, en este caso por ejemplo, lo importante es que la fecha tenga el formato apropiado.
Se decidió entonces atacar los problemas más específicos:

http://schema.org/ratingValue has rdfs:domain different from: http://schema.org/Rating

Estrategia:
Primero hubo que investigar, cuáles son todos los dominios para esa propiedad que aparecen en el dataset, para ver si se pueden 
acomodar. Esto se realizó con la siguiente consulta sparql:

\begin{lstlisting}[frame=single]  
select distinct ?dominio (count (?s) as ?cantidad) 
where{
?s <http://schema.org/ratingValue> ?value .
?s a ?dominio .
}GROUP BY ?dominio
\end{lstlisting}

Y los resultados fueron

\begin{tabular}{| l | c |}
 <http://schema.org/Rating> & 120165\\
 <http://schema.org/AggregateRating> & 54815 \\
 <http://schema.org/Review> & 5867 \\
 <http://schema.org/Product> & 331 \\
\end{tabular}

Se puede apreciar que en la mayor parte de los casos de error, se dieron porque se incluyó la propiedad ratingValue directamente en 
el Review. Ahora surge la necesidad de saber si esos casos donde el Review tiene la propiedad ratingValue, existe una propiedad reviewRating 
con su respectivo Rating.
Se relizó la consulta:

\begin{lstlisting}[frame=single]  
select (count (?s) as ?cantidad) 
where{
?s a <http://schema.org/Review> . 
?s <http://schema.org/ratingValue> ?value . 
?s <http:schema.org/reviewRating> ?rating . 
}
\end{lstlisting}

El resultado fue 0.

Por lo tanto ese problema se solucionó con la siguiente consulta:

\begin{lstlisting}[frame=single]  
DELETE { 
GRAPH ?g{ 
?s <http://schema.org/ratingValue> ?value . 
} 
} 
INSERT{ 
GRAPH ?g{ 
?s <http://schema.org/reviewRating> ?rating . 
?rating a <http://schema.org/Rating> . 
?rating <http://schema.org/ratingValue> ?value . 
}
}
WHERE{
GRAPH ?g {
?s a <http://schema.org/Review> .
?s <http://schema.org/ratingValue> ?value .
}
}
\end{lstlisting}

schema:worstRating and schema:bestRating has rdfs:domain different from: http://schema.org/Rating
Estrategia:
El proceso es igual al anterior y los dominios disintos de rating eran Review, la misma cantidad que en el anterior 5867

Resultados: 
Idem los anteriores.

http://schema.org/itemReviewed is missing proper range

Estrategia:
Esta evaluación se refiere a que existen recursos objeto de dicha propeidad, que no tienen un tipo específico. Para ver si 
puede ser resuelto, primer hubo que evaluar qué clase de recursos contienen como objeto, esas propiedades que presentaron problemas.

Resultado:
La evaluación encontró que, todos esos recursos eran URIs de un dominio web 411ca.com para ser derreferenciados. La solución podría haber sido 
descargar dichos recursos y anexarlos a la propiedad, pero lamentablemente dichos recursos no se encontraban disponibles de forma online.

Curado Evaluación nº 3 - RDFUnit Manual:

{DateProperty} has a format different to YYYY-MM-DD:
Para todas las propiedades que indiquen una fecha, los pasos para reparar este caso son los mismos.

Cuando nos encontramos con este caso, se requiere de otra evaluación para entender cómo encarar el problema. Es una evaluación cuyo resultado
pueda responder la siguiente pregunta:

Si no tiene formato YYYY-MM-DD, ¿Qué formato tiene?

La idea en esta evaluación entonces, es encontrar qué formatos distintos contienen los datos, para poder trasladarlos al correcto.

Estrategia:

Comenzando con la siguiente consulta:


\begin{lstlisting}[frame=single]  
SELECT distinct ?date 
WHERE{
?review <http://local.org/id> ?id .
?review {DateProperty} ?date .
FILTER (!REGEX(str(?date), "^[0-9]{4}-[0-9]{2}-[0-9]{2}$", "i")) .
FILTER NOT EXISTS{
?review <http://local.org/deplicateOf> ?dup.
}
}
\end{lstlisting}



Esta consulta retorna la fecha de todos los reviews que no estén duplicados y que además dicha fecha no tenga el formato YYYY-MM-DD

Luego observando los resultados manualmente, se prsta atención also primeros resultados y se trata de encontrar con qué patrón están formados 
para luego anotarlo y realizar la misma consulta esta vez filtrando los reusltados ocn dicho patrón. Iterando este proceso hasta que 
queden 0 resultados.

Resultados:

Se descubrieron los siguientes patrones
\\
\\
http://schema.org/datePublished
\begin{tabular}{| l | c |}
Expresión regular & Cantidad de reviews\\
\verb|^[A-z]+ [0-9]{2}, [0-9]{4}$ &| 14715 \\
\verb|^[0-9]{4}-[0-9]{2}-[0-9]{2}.+ &| 17700 \\
\verb|^[0-9]{4}-[0-9]{2}-[0-9]{1} [0-9]{2}:[0-9]{2}:[0-9]{2}$ &| 7179 \\
\verb|^[0-9]{2}\\.[0-9]{2}\\.[0-9]{4}$ &| 7172\\
\verb|^[A-z]+ [0-9]{2}, [0-9]{4}.+ &| 5049\\
\verb|^[A-z]{3} [0-9]{1}, [0-9]{4} &| 1847\\
\verb|^[0-9]{2}\\.[0-9]{1}\\.[0-9]{4}$ &| 410\\
\verb|^[0-9]{1}\\.[0-9]{1}\\.[0-9]{4}$ &| 120\\
\verb|[0-9]{2}/[0-9]{2}/[0-9]{2} &| 67\\
\verb|^[0-9]{1}\\.[0-9]{2}\\.[0-9]{4}$ &| 30
\end{tabular}

http://purl.org/dc/terms/date
\begin{tabular}{| l | c |}
Expresión regular & Cantidad de reviews\\
\verb|^[0-9]{4}-[0-9]{2}-[0-9]{2}.+ &| 33833\\
\verb|^[A-z]+.[A-z]{2}.[0-9]{4}-[0-9]{2}-[0-9]{2}$ &| 33058\\
\verb|^[A-z]{3}\\n.*[0-9]{1}\\n.*[0-9]{4}$ &| 25543\\
\verb|^[A-z]{3}\\n.*[0-9]{2}\\n.*[0-9]{4}$ &| 17770\\
\verb|^[A-z]+ [0-9]{2}, [0-9]{4}$ &| 17069\\
\verb|[0-9]{2}((?!-).)[0-9]{2}((?!-).)[0-9]{4} &| 11720\\
\verb|^[0-9]{2} [A-zéû]+ [0-9]{4} &| 4147\\
\verb|^[0-9]{2}((?!-).)[0-9]{2}((?!-).)[0-9]{4} &| 4093\\
\verb|^[0-9]{2}((?!-).)[0-9]{1}((?!-).)[0-9]{4} &| 2803\\
\verb|^[A-z]{3} [0-9]{1}, [0-9]{4} &| 2783\\
\verb|^[A-z]+ [0-9]{2}[a-z]{2} [0-9]{4}\\.$ &| 2058\\
\verb|^[0-9]{2} [A-z]{3} [0-9]{4} &| 1730\\
\verb|^[0-9]{1}((?!-).)[0-9]{1}((?!-).)[0-9]{4}$. &| 1103\\
\verb|^[0-9]{2}-[0-9]{2}-[0-9]{4}$ &| 1024\\
\verb|^[A-z]+ [0-9]{1}[a-z]{2} [0-9]{4}\\.$ &| 746\\
\verb|^[0-9]{1}((?!-).)[0-9]{2}((?!-).)[0-9]{4}$ &| 279\\
\end{tabular}\\

http://schema.org/publishDate
\begin{tabular}{| l | c |}
Expresión regular & Cantidad de reviews\\
\verb|[0-9]{2}/[0-9]{2}/[0-9]{2} &| 2511\\
\verb|^[0-9]{2}\\.[0-9]{2}\\.[0-9]{4}$ &| 2207
\end{tabular}\\

http://schema.org/dtreviewed
\begin{tabular}{| l | c |}
Expresión regular & Cantidad de reviews\\
\verb|^[0-9]{1}((?!-).)[0-9]{2}((?!-).)[0-9]{4}$ &| 14762\\
\verb|^[0-9]{1}((?!-).)[0-9]{1}((?!-).)[0-9]{4}$ &| 6358\\
\verb|^[0-9]{2}((?!-).)[0-9]{2}((?!-).)[0-9]{4}$ &| 4193\\
\verb|^[0-9]{2}((?!-).)[0-9]{1}((?!-).)[0-9]{4}$ &| 1590
\end{tabular}

Una vez encontrados los patrones, sólo fue cuestion de correr un algoritmo en Java dónde se contemplan todo los patrones y se 
le aplica la correción apropiada al String y se vuelve a almacenar en el review.
También en varios casos hubo que verificar dónde se encontraba el día y dónde el mes como por ejemplo para casos como éste 
\verb|^[0-9]{2}\\.[0-9]{2}\\.[0-9]{4}\$ &| 2207 
Manualmente se observan los resultados de ese patrón y se puede observar si los primeros 2 números son mes o día.
También hubo casos no tratables como \verb|^[A-z]+ [0-9]{2}| donde no estaba explicitado el año.

\verb|http://purl.org/stuff/rev#rating is not in the expected range (1-5) & 15319 &| 298458\\


{RatingProperty} is not in the expected range (1-5):
Cuando nos encontramos con un review en esta situación, resulta casi imposible incluso manualmente corregir este error con la información 
provista por el review o el documento que contiene el review. Si el rating del review es 7, ¿Cuál es el máximo valor que el autor tenía en mente cuando  
generó la evaluación?. Uno podría pensar que lo más lógico sería 10, pero no se puede saber a ciencia cierta, por lo menos con la información del entorno al review.
Para encontrar la respuesta, se necesita la información de todos los reviews del dominio que contiene el review problemático. 

Estrategia:
Para resolver el problema de rango en los ratings de los reviews entonces, lo primero que hay que hacer es una evaluación, en la cual 
se pueda observar qué valores tomaron los ratings de otros reviews correspondientes al mismo sitio web hosting del review. 
Esto sólo se aplicó par ala ontología purl, dado que apra schema los valores erróneos eran muy pocos y no se jsutificaba.
Para implementarlo, lo primero que se realizó fue la siguiente consulta

\begin{lstlisting}[frame=single]
 PREFIX xsd: <http://www.w3.org/2001/XMLSchema#> 
 SELECT distinct ?g ?rating ?id 
 WHERE{
 GRAPH ?g{
 ?review <http://local.org/id> ?id .
 ?review <http://purl.org/stuff/rev#rating> ?rating .
 FILTER NOT EXISTS{
 ?review <http://local.org/duplicateOf> ?dup .
 }
 FILTER NOT EXISTS{
 ?rat <http://purl.org/stuff/rev#maxRating> ?best .
 }
 FILTER NOT EXISTS{
 ?rat <http://purl.org/stuff/rev#minRating>  ?worst .
 }
 FILTER ((xsd:integer(?rating) > 5) || (xsd:integer(?rating) < 1)).
 }
 }
\end{lstlisting}

Esta consulta retorna todos las urls de los documentos que contienen el review problemático incluyendo el id del review y el 
rating correspondiente a dicho review.

A partir de esta información, puede obtenerse el dominio de la URL, y almacenarse toda la información en un mapa de Java, donde para cada dominio se 
guarde los valores de los ratings encontrados, y la cantidad de veces que se encontró cada valor de rating.

Resultados:
El mapa resultante de esta evaluación fue el siguiente.

Dominio: www.yellowbot.com\\
\begin{tabular}{| l | c |}
 Valor & Cantidad\\
 0 & 13\\
\end{tabular}

Dominio: www.chip.de\\
\begin{tabular}{| l | c |}
 Valor & Cantidad\\
 6 & 1\\
 8 & 2\\
 9 & 1\\
 19 & 1\\
 25 & 1\\
 27 & 1\\
 29 & 1\\
 31 & 1\\
 30 & 1\\
 34 & 1\\
 33 & 2\\
 38 & 1\\
 36 & 3\\
 37 & 1\\
 42 & 1\\
 43 & 3\\
 40 & 1\\
 46 & 1\\
 47 & 4\\
 44 & 1\\
 45 & 4\\
 51 & 9\\
 50 & 10\\
 49 & 8\\
 48 & 6\\
 55 & 7\\
 54 & 15\\
 53 & 5\\
 52 & 12\\
 59 & 8\\
 58 & 18\\
 57 & 11\\
 56 & 19\\
 63 & 25\\
 62 & 23\\
 61 & 12\\
 60 & 12\\
 68 & 62\\
 69 & 34\\
 70 & 50\\
 71 & 62\\
 64 & 24\\
 65 & 39\\
 66 & 35\\
 67 & 32\\
 76 & 82\\
 77 & 82\\
 78 & 81\\
 79 & 86\\
 72 & 81\\
 73 & 70\\
 74 & 72\\
 75 & 73\\
 85 & 120\\
 84 & 92\\
 87 & 128\\
 86 & 91\\
 81 & 97\\
 80 & 117\\
 83 & 115\\
 82 & 136\\
 93 & 52\\
 92 & 68\\
 95 & 49\\
 94 & 58\\
 89 & 98\\
 88 & 103\\
 91 & 91\\
 90 & 93\\
 100 & 101\\
 98 & 11\\
 99 & 8\\
 96 & 34\\
 97 & 27
\end{tabular}

Dominio: www.gamezebo.com\\
\begin{tabular}{| l | c |}
 Valor & Cantidad\\
 0 & 5\\
\end{tabular}

Dominio: www.gogobot.com\\
\begin{tabular}{| l | c |}
 Valor & Cantidad\\
 0 & 1\\
\end{tabular}

Dominio: www.ormigo.com\\
\begin{tabular}{| l | c |}
 Valor & Cantidad\\
 0 & 9\\
 -1 & 24
\end{tabular}

Dominio: www.chroniclesofchaos.com\\
\begin{tabular}{| l | c |}
 Valor & Cantidad\\
 0 & 17\\
 6 & 391\\
 7 & 776\\
 8 & 851\\
 9 & 497\\
 10 & 101\\
\end{tabular}

Dominio: www.kollermedia.at\\
\begin{tabular}{| l | c |}
 Valor & Cantidad\\
 6 & 19\\
 7 & 26\\
 8 & 26\\
 9 & 7\\
 10 & 7\\
\end{tabular}
 
En base a esta información se infirieron los siugientes rangos:

\begin{tabular}{| l | c | r | }
 Dominio & Rango mínimo & Rango máximo \\
 www.gamezebo.com & 0 & 5 \\
 www.yellowbot.com & 0 & 5 \\
 www.chroniclesofchaos.como & 0 & 10 \\
 www.kollermedia.at & 1 & 10 \\
 www.chip.de & 1 & 100 
\end{tabular}

Luego estos resultados se corroboraron manualmente revisando todos los valores que poseía el dominio tanto dentro del rango 1-5 como fuera .
Desde ya que no se puede asegurar la veracidad de los rangos encontrados, dado que por ejemplo, podría suceder que para el dominio 
www.kollermedia.at por mas absurdo que parezca el rango real es 1-11 pero no existe ningún review con el valor 11 en su rating.
Nota: Manualmente se verificó que ormigo poseía simplemente reviews mal generados.

Una vez con estos resultados se puede proceder a corregir los valores de los reviews del dataset.

Estrategia:
Disponiendo de los rango correctos para cada dominio se recorrieron todos los reviews de cada dominio resultante, y se les adicionaron 
las propiedades rev:minRating y rev:maxRating con los valores correspondientes.
Esto no sólo para los reviews con ratings erróneos encontrados en la etapa de evaluación (fuera del rango 1-5) dado que, para los reviews de 
cada dominio con rango encontrado, por más que el rating se encuentre dentro del rango 1-5 y aparente ser correcto,  ya se sabe que dicho dominio se 
maneja con rangos diferentes y el valor real del rating cambiaría (no es lo mismo el valor 3 en un rango 1-100 que en un rango 1-5)

Resultados:
Se le asignaron propiedades a:
\begin{tabular}{| l | c |}
 www.gamezebo.com & 3905\\
 www.yellowbot.com & 8955\\
 www.chroniclesofchaos.como & 4966\\
 www.kollermedia.at & 9968\\
 www.chip.de & 13751\\
 Total & 41545
\end{tabular}

