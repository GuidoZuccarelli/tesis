\chapter{Recolección de Datos}
\label{chapter:recoleccion}

\section{Objetivo}
\label{section:recoleccion-objetivo}

Como se indicó anteriormente la información semántica que va a ser necesaria para construir se encuentra en la web en forma de documentos 
HTML que tienen la particularidad de ser muy efímeros, de manera tal que un documento que poseía datos relevantes a la fecha,
puede al día siguiente, o dejar de estar disponible on-line, o haber cambiado de forma tal que la información de éste ya no es 
relevante, o ya no la posee. 

En [] sección 4.2 Challenges for the selection of data sources se generó una estadística de este caso, donde se estableció que 
en promedio 62\% de los documentos entoncontrados, continuaban on-line luego de un año, y de estos, sólo un 56\% aún poseían 
datos relevantes. 

Si bien armar un dataset con sólo información extraída de los documentos sin descargar estos últimos es posible, la situación anterior 
genera la necesidad de mantenerlos en una copia local para evitar una posible pérdida de los mismos. 

El objetivo entonces será armar un repositorio local con los documentos on-line descargados que se cree que tienen la información 
necesaria. 

 
\section{Estrategia}
\label{section:recoleccion-estrategia}

Se optó por la utilización de Sindice como fuente de datos. Dado que contiene indexados suficiente cantidad de documentos de las 
ontologías a utilizar para hacer una prueba del caso.
La consulta arrojó: 10.216.632 documentos que contenían la clase purl:Review y 394.533 que contenían la clase schema:Review
Luego en base a que a comienzos del año sindice limitó la paginación de sus consulas a 100. De manera que sólo se puede acceder a 5000 
resultados se planificó la siguiente estrategia:

Primero se realizó una consulta de los documentos para cada ontología con los reusltados agrupados por dominio (el sitio dueño del documento) ordenada en orden descendiente por cantidad de docuemntos 

Luego a mano se inspeccionaron y seleccionaron los cuarenta dominios con mayor cantidad de documentos para la ontología que aún conservaban intactos sus documentos

Y luego se ejecutó una consulta para recuperar hasta 5000 URLs de documentos por cada uno de esos 40 dominios.

Esto generó una lista de URLs a la cuál se le realizó un crawling, para descargar el documento actual de la web por cada uno. Para realizar la descarga se utilizó la librería fluent.

\section{Resultados}
\label{section:recoleccion-resultados}

Se obtuvieron de las consultas a Sindice 254950 urls de documentos potencialmente contenedores de reviews.

De las 254950 existían 236697 accesibles.

De los 236697 51 documentos tenían una url de más de 255 caracteres por lo que no se pudo almacenar en el disco utilizando la url como nombre del archivo. Y 6 documentos malformados.

La estadística fue la siguiente:

\begin{tabular}{| l | c | }
   Respuesta HTTP & Cantidad de documentos \\
   200 & 236697 \\
   Error sin código & 10026 \\
   408 & 3519 \\
   500 & 2963 \\
   400 & 77 \\
   403 & 25 \\
   Connection reset & 19 \\
   Premature EOF & 8 \\
   Server redirected too many times & 8 \\
   504 & 7 \\
   Total & 254950 \\
 \end{tabular}

